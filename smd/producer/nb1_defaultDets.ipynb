{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2984837c",
   "metadata": {},
   "source": [
    "# smalldata_tools intro & links\n",
    "smalldata_tools is a package of analysis tools build on MPIDataSource and the psana.Detector class\n",
    "It supports analysis in two modes: smalldata/event-based & cube/binned (the latter needs to be expanded for LCLS-2)\n",
    "For the event-based mode, the two most important concepts for hdf5 production are the \"DefaultDetector\" and the \"DetObject\" for experiment specific data reduction. These will be covered in these notebooks.\n",
    "Documentation is available on [confluence](https://confluence.slac.stanford.edu/display/PSDM/smalldata_tools%3A+Analysis+tools+for+aligned+data)\n",
    "We are also starting to build out [notebooks](https://confluence.slac.stanford.edu/display/PSDM/2.+Example+analysis+notebooks) to help with analyses of particular measurement-types or advances uses of the hdf5 files (summing of data over runs w/ proper error treatment etc).\n",
    "\n",
    "This example demonstrates what the code in the producer-files does which are used in the standard ARP-based smalldata production. The actual producer files contain a lot more lines for a variety of convenience options etc in addition to the 'main' ideas shown here.\n",
    "smalldata_tools is based on top on MPIDataSource which allows us to write code in a simple loop, test it on a single core, but be able to run it with MPI on hundreds of cores without any code changes.\n",
    "\n",
    "MPIDataSource will write out a hdf5 file with event-based (timesorted for LCLS1!) and 'summed'/single datasets. Some of the standard data (e.g. GEM, BLD, event codes) will be included by default.\n",
    "\n",
    "# Add default Detectors \n",
    "Now we use the first concept of smalldata_tools: expand the list of always saved fields to regularly used detectors where the event based data is small. If applicable, use name colloquially used by the beamline scientists (ipm2, ...)\n",
    "Translate technical event codes (which vary by hutch and possible even experiment) to standard binary flags: lightStatus/[xray,laser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76497dd8-09f6-4e91-b471-93469a07ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "from tqdm import tqdm\n",
    "import h5py as h5\n",
    "from pathlib import Path\n",
    "import tables\n",
    "import sys\n",
    "\n",
    "import psana as ps\n",
    "sys.path.append('/sdf/group/lcls/ds/tools/smalldata_tools/latest')\n",
    "from smalldata_tools.SmallDataUtils import defaultDetectors, detData\n",
    "\n",
    "run = 320 \n",
    "exp = 'xpptut15'\n",
    "\n",
    "dsname = 'exp={}:run={}'.format(exp,run,exp[:3],exp)\n",
    "\n",
    "ds = ps.MPIDataSource(dsname)\n",
    "\n",
    "small_data = ds.small_data('./UserMtg24.h5', gather_interval=5)\n",
    "default_dets = defaultDetectors(exp[:3])\n",
    "\n",
    "max_evt = 5\n",
    "ds.break_after(max_evt) # stop iteration after max_evt events (break statements do not work reliably with MPIDataSource).\n",
    "for nevt,evt in enumerate(ds.events()):\n",
    "    if nevt>=max_evt:\n",
    "        break\n",
    "\n",
    "    def_data = detData(default_dets, evt)\n",
    "    \n",
    "    small_data.event(def_data)\n",
    "small_data.close()\n",
    "\n",
    "dat_def = tables.open_file('./UserMtg24.h5').root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a754b5-21e7-44c8-9f78-c63e3a491f5a",
   "metadata": {},
   "source": [
    "# Read the file\n",
    "Print some of the values of the 'default' data saved in the hdf5 files.\n",
    "lightStatus/[xray,laser] are avaialble for all hutches - the typically used event codes are setup in smalldata_tools\n",
    "tt/ttCorr(etc) are also available for all instrumented where a timetool has been integrated.\n",
    "ipm2/sum is i0-data measured a common component device, the names of the typically use device will differ hutch-by-hutch and may also change from experiment to experiment (though there typically are only 2-3 options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('laser:', dat_def.lightStatus.laser.read())\n",
    "print('i0 (ipm2):',dat_def.ipm2.sum.read())\n",
    "print('timetool:',dat_def.tt.ttCorr.read())\n",
    "dat_def.ipm2.sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad2761",
   "metadata": {},
   "source": [
    "# Add the timetool waveform to the default detectors\n",
    "There are a few default detectors that could be adjusted (e.g. giving the analog inputs special names).\n",
    "Another option would be to apply a better calibration to the timetool (here, we take calibration runs to translate a pixel position into a ps measurement.\n",
    "The most typically (though still rarely) needed detector of this type is the raw timetool traces. This is only needed is something has gone wrong in the online analysis, otherwise simply use the values in tt(/ttCorr) \n",
    "\n",
    "This example also demonstrates how to retrieve the data to be stored in the hdf5 file, process it and store the new result as well. \n",
    "\n",
    "Thr ttRaw detector extracts the timetool traces as defined in the DAQ. In this example, we also add a new parameter 'myVar' to the 'ttRaw' dictionary (obviously an example for more meaninful code....)\n",
    "\n",
    "The timetool reprocessing code in smalldata_tools have will do nothing until a core gets a reference, then I'll run the fitting code on the ratio. We can add other code to do this here as well.\n",
    "\n",
    "tqdm gives you a rough idea of how fast the job will run (although the comparison is not really perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b305ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smalldata_tools.SmallDataDefaultDetector import ttRawDetector\n",
    "\n",
    "ttRawDet = ttRawDetector(env=ds.env())\n",
    "#ttRawDet.setPars({'beamOff':[-137], 'refitData': True, 'kind': 'stepUp'})\n",
    "ttRawDet.setPars({'beamOff':[162]}) #old data before SXR beam sharing, \n",
    "default_dets.append(ttRawDet)\n",
    "\n",
    "small_data = ds.small_data('./UserMtg_smd_deftt.h5', gather_interval=5)\n",
    "\n",
    "max_evt = 5 #because this fails when writing the file. \n",
    "userDict = {}\n",
    "for nevt,evt in tqdm(enumerate(ds.events())):\n",
    "    if nevt>=max_evt:\n",
    "        break\n",
    "\n",
    "    def_data = detData(default_dets, evt)\n",
    "\n",
    "    #retrieve data for for further processing in this event loop\n",
    "    ttRaw_wf = def_data['ttRaw']['tt_signal']\n",
    "    \n",
    "    #store the result you'd like to keep\n",
    "    def_data['ttRaw']['myVar'] = np.nanmax(ttRaw_wf)\n",
    "    \n",
    "    small_data.event(def_data)\n",
    "small_data.close()\n",
    "\n",
    "dat_deftt = tables.open_file('./UserMtg_smd_deftt.h5').root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959fbe7",
   "metadata": {},
   "source": [
    "## Check default data - timetool trace as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08542293",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Points(dat_deftt.ttRaw.tt_signal.read(-1).squeeze(), label='signal').options(width=800) *\\\n",
    "hv.Points(dat_deftt.ttRaw.tt_reference.read(-1).squeeze(),label='references').options(width=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
